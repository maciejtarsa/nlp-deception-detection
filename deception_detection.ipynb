{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deception_detection.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOK3JZKvslVTbs9xwRToXM5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"D_PBYPrW1gZF","executionInfo":{"status":"ok","timestamp":1636723941657,"user_tz":0,"elapsed":212,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["import csv\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support\n","from nltk.classify import SklearnClassifier\n","from random import shuffle\n","from sklearn.pipeline import Pipeline\n","import re"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvEBZLkS6l_l"},"source":["## Initial functions and simple setup"]},{"cell_type":"code","metadata":{"id":"sBVtruTB4h6w","executionInfo":{"status":"ok","timestamp":1636723941989,"user_tz":0,"elapsed":8,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# a function to load data from a file and append it to the rawData\n","def loadData(path, Text=None,):\n","    with open(path) as f:\n","        reader = csv.reader(f, delimiter='\\t')\n","        \n","        for line in reader:\n","            # skip the header\n","            if line[0] == \"DOC_ID\":\n","                continue\n","            # create a tuple with data information\n","            (Id, Text, Label) = parseReview(line)\n","            rawData.append((Id, Text, Label))"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"qXfTEnL05CUz","executionInfo":{"status":"ok","timestamp":1636723941989,"user_tz":0,"elapsed":7,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# a function to split the data between trainData and testData\n","def splitData(percentage, updated=False, ngram=1): \n","    dataSamples = len(rawData)\n","    halfOfData = int(len(rawData)/2)\n","    trainingSamples = int((percentage*dataSamples)/2)\n","    if updated == False:\n","        for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n","            trainData.append((toFeatureVector(preProcess(Text)),Label))\n","        for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n","            testData.append((toFeatureVector(preProcess(Text)),Label))\n","\n","    elif updated == True:\n","        for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n","            trainData.append((toFeatureVector_updated(preProcess_updated(Text, ngram)),Label))\n","        for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n","            testData.append((toFeatureVector_updated(preProcess_updated(Text, ngram)),Label))"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kyVx13t5MsO","executionInfo":{"status":"ok","timestamp":1636723941990,"user_tz":0,"elapsed":7,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# a function to convert a line from input file into id/text/label tuple\n","def parseReview(reviewLine):\n","  \n","    # convert id to an integer\n","    id = int(reviewLine[0])\n","    text = reviewLine[8]\n","    label_text = reviewLine[1]\n","    # convert the label to either real or fake\n","    label = 'fake' if label_text == '__label1__' else 'real'\n","\n","    return (id, text, label)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzyOrI4v5tsn","executionInfo":{"status":"ok","timestamp":1636723941990,"user_tz":0,"elapsed":6,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# turn review text inot a list of tokens\n","# includes some preprocessing methods\n","def preProcess(text):\n","    \n","    # adds space before punctuation signs\n","    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n","    # adds space after punctuation signs\n","    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)\n","    # splits on multiple occurances of white spaces\n","    tokens = re.split(r\"\\s+\",text)\n","    # converts all tokens to lower case\n","    tokens = [t.lower() for t in tokens]\n","    # returns the produced tokens\n","    return tokens"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsEP6wmd6AJL","executionInfo":{"status":"ok","timestamp":1636723941991,"user_tz":0,"elapsed":7,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# a function to convert tokens to feature vectors\n","# returns a dictionary containing features as keys\n","# and counts as values\n","def toFeatureVector(tokens):\n","\n","    # create an empty dictionary to be returned at the end\n","    v = {}\n","    # for each token\n","    for t in tokens:\n","        # check if that token already exists in the local dictionary\n","        try:\n","            i = v[t]\n","            # if it does, increment the count\n","            v[t] = i + 1\n","        # otherwise, assign a count of 1\n","        except KeyError:\n","            v[t] = 1\n","\n","    # return the dictionary\n","    return v"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"m9Qi0NCT6adg","executionInfo":{"status":"ok","timestamp":1636723942274,"user_tz":0,"elapsed":289,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# a function for training and validating the classifier\n","def trainClassifier(trainData):\n","    print(\"Training Classifier...\")\n","    pipeline =  Pipeline([('svc', LinearSVC())])\n","    return SklearnClassifier(pipeline).train(trainData)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vx9LYye16obE","executionInfo":{"status":"ok","timestamp":1636723942274,"user_tz":0,"elapsed":11,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# a function to cross validate\n","# takes the dataset as input and a value of folds to be used\n","# it returns 4 metric averages over all folds\n","def crossValidate(dataset, folds = 10):\n","    \n","    # shuffle the dataset\n","    shuffle(dataset)\n","    # create an empty dictionary for the results\n","    results = {}\n","    foldSize = int(len(dataset)/folds)\n","    # set up empty lists for precisions, recalls, f1 scores and accuracies\n","    precisions = []\n","    recalls = []\n","    f1_scores = []\n","    accuracies = []\n","\n","    # create an empty list to store the division of indices\n","    i_list=[]\n","    # divide the dataset into segments\n","    for i in range(0,len(dataset),foldSize):\n","        i_list.append(i)\n","    # append the length as the last item\n","    i_list.append(len(dataset))\n","\n","    # iterate through that list\n","    for i in range(len(i_list)-1):\n","        # and set up training data (9 out of 10 segments)\n","        training_data = trainData[:i_list[i]] + trainData[i_list[i+1]:]\n","        # and validation data (1 out of 10 segments)\n","        validation_data = trainData[i_list[i]:i_list[i+1]]\n","\n","        # train the model using that training data\n","        model = trainClassifier(training_data)\n","        # obtain validation labels from the validation data\n","        validation_labels = [_[1] for _ in validation_data]\n","        # predict the validation data using trained model\n","        predictions = predictLabels(validation_data, model)\n","\n","        # append the values for precision, recall, f1 score and accuracy\n","        # for each iteration over the segments\n","        precisions.append(precision_score(validation_labels, predictions, average='weighted'))\n","        recalls.append(recall_score(validation_labels, predictions, average='weighted'))\n","        f1_scores.append(f1_score(validation_labels, predictions, average='weighted'))\n","        accuracies.append(accuracy_score(validation_labels, predictions))\n","\n","    # append the averages of each score to the dictionary\n","    results['precision'] = sum(precisions)/len(precisions)\n","    results['recall'] = sum(recalls)/len(recalls)\n","    results['f1_score'] = sum(f1_scores)/len(f1_scores)\n","    results['accuracy'] = sum(accuracies)/len(accuracies)\n","    # return the values in the dictionary\n","    return results"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"doryny667P7J","executionInfo":{"status":"ok","timestamp":1636723942275,"user_tz":0,"elapsed":11,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# a function to predict labels\n","def predictLabels(reviewSamples, classifier):\n","    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n","\n","# a function to predict a single label\n","def predictLabel(reviewSample, classifier):\n","    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ik79q_bQ-6fr"},"source":["The functions below consier additional pre-processing methods aimed at improving the effectiveness of the classifier."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YwR34iJa-wj1","executionInfo":{"status":"ok","timestamp":1636723942275,"user_tz":0,"elapsed":10,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}},"outputId":"708c2110-be9e-4cbd-946b-a5ff0480d462"},"source":["# import stopwords list from NLTK\n","import nltk\n","# from nltk.corpus import stopwords\n","# nltk.download('stopwords')\n","# stop_words = set(stopwords.words('english'))\n","# define stemmer for stemming\n","stemmer = nltk.stem.PorterStemmer()\n","# define stemmer for lemmatising\n","lemmatiser = nltk.stem.WordNetLemmatizer()\n","# import wordnet words for lemmatiser\n","nltk.download('wordnet')"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"kUpXTlmb7YWu"},"source":["## Main procedure\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W78Rp7pk-y_8"},"source":["### Simple setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QH5QH0Av7aqK","executionInfo":{"status":"ok","timestamp":1636723991020,"user_tz":0,"elapsed":48751,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}},"outputId":"fafc77ae-2753-4118-8ce5-20cc747379bc"},"source":["# loading reviews\n","# initialise global lists that will be appended to by the methods below\n","rawData = []\n","trainData = []\n","testData = []\n","\n","# input file location\n","reviewPath = './amazon_reviews.txt'\n","\n","## Run the functions\n","print(f\"Preparing the dataset\")\n","\n","# load the data\n","loadData(reviewPath) \n","\n","# split data into 80% training and 20% test\n","splitData(0.8)\n","\n","print(f\"Number of reviews in raw data: {len(rawData)}\")\n","print(f\"Number of reviews in training data: {len(trainData)}\")\n","print(f\"Number of reviews in test data: {len(testData)}\")\n","\n","# cross validate on training data\n","cv_results = crossValidate(trainData)\n","# print the results\n","print(\"Results of cross validation:\")\n","print(\"Precision: %f\" % cv_results['precision'])\n","print(\"Recall: %f\" % cv_results['recall'])\n","print(\"F1 score: %f\" % cv_results['f1_score'])\n","print(\"Accuracy: %f\" % cv_results['accuracy'])"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Preparing the dataset\n","Number of reviews in raw data: 21000\n","Number of reviews in training data: 16800\n","Number of reviews in test data: 4200\n","Training Classifier...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Results of cross validation:\n","Precision: 0.614237\n","Recall: 0.614048\n","F1 score: 0.613918\n","Accuracy: 0.614048\n"]}]},{"cell_type":"markdown","metadata":{"id":"JherxonJ9Bd6"},"source":["#### Evaluate using the test set\n","We can now train the classifier on 80% of the data and evaluate its performance using the 20% of the data we left out earlier."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q1uak4z69Aps","executionInfo":{"status":"ok","timestamp":1636723996680,"user_tz":0,"elapsed":5667,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}},"outputId":"6cf17d99-9fc4-467e-f8c8-eb5c680f280a"},"source":["# train the classifier using training data\n","classifier = trainClassifier(trainData)\n","# get the ground-truth labels from the data\n","testTrue = [t[1] for t in testData]\n","# classify the test data to get predicted labels   \n","testPred = predictLabels(testData, classifier) \n","# evaluate\n","finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted')\n","accuracy = accuracy_score(testTrue, testPred)\n","print(f\"Done training!\")\n","print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n","print(f\"Accuracy: {accuracy}\")"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Classifier...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Done training!\n","Precision: 0.603594\n","Recall: 0.603571\n","F Score:0.603550\n","Accuracy: 0.6035714285714285\n"]}]},{"cell_type":"markdown","metadata":{"id":"DPAVl6Rq-3MN"},"source":["### Additional pre-processing and token weights"]},{"cell_type":"markdown","metadata":{"id":"qxaM4xYGU3Pz"},"source":["Amended relevant functions below:"]},{"cell_type":"code","metadata":{"id":"-kZVY7XU_EkQ","executionInfo":{"status":"ok","timestamp":1636723996681,"user_tz":0,"elapsed":14,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# turn review text inot a list of tokens\n","# includes additional preprocessing methods\n","# apostrophese removal, stopwords removal, punctuation removal,\n","# stemming, lemmatising, n-grams\n","def preProcess_updated(text, ngram=1):\n","    \n","    \"\"\"\n","    \n","    # removal of apostrophes\n","    # replace \"'\" with \"\"\n","    text.replace(\"'\", \"\")\n","    \"\"\"\n","\n","    # adds space before punctuation signs\n","    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n","    # adds space after punctuation signs\n","    text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)\n","    # splits on multiple occurances of white spaces\n","    tokens = re.split(r\"\\s+\",text)\n","    # converts all tokens to lower case\n","    tokens = [t.lower() for t in tokens]\n","    \n","    \"\"\"\n","    # removal of stopwords\n","    tokens = [token for token in tokens if not token in stop_words]\n","    \n","    # removal of punctuation\n","    tokens = [token for token in tokens if token.isalpha()]\n","    \"\"\"\n","    \n","    # stem the tokens\n","    tokens = [stemmer.stem(token) for token in tokens]\n","    \n","    # lemmatise the tokens\n","    tokens = [lemmatiser.lemmatize(token) for token in tokens]\n","    \n","    # an attempt to create bigrams and use them for training\n","    if (ngram == 2):\n","        new_tokens = []      \n","        tokens = ['<s>'] + tokens + ['</s>']\n","        for i in range(len(tokens)-1):\n","            new_tokens.append(tokens[i] + ' ' + tokens[i+1])\n","        tokens = new_tokens\n","\n","   # an attempt to create trigrams and use them for training\n","    if (ngram == 3):\n","        new_tokens = []      \n","        tokens = ['<s>'] + tokens + ['</s>']\n","        for i in range(len(tokens)-2):\n","            new_tokens.append(tokens[i] + ' ' + tokens[i+1] + ' ' + tokens[i+2])\n","        tokens = new_tokens\n","\n","    # returns the produced tokens\n","    return tokens"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"XFvGRyCI_h4w","executionInfo":{"status":"ok","timestamp":1636723996682,"user_tz":0,"elapsed":13,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# a function to convert tokens to feature vectors\n","# returns a dictionary containing features as keys\n","# and weights as values\n","# this updated function returns weights instead of counts\n","def toFeatureVector_updated(tokens):\n","\n","    # create an empty dictionary to be returned at the end\n","    v = {}\n","    # for each token\n","    for t in tokens:\n","        # check if that token already exists in the local dictionary\n","        try:\n","            i = v[t]\n","            # if it does, increment the count\n","            v[t] = i + 1\n","        # otherwise, assign a count of 1\n","        except KeyError:\n","            v[t] = 1\n","\n","    # for every item in the dictionary, turn counts into weights\n","    v_weighted = {key:value/sum(v.values()) for (key, value) in v.items()}\n","\n","    # return the ditionary\n","    return v_weighted"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4WpXeCk6UrpX"},"source":["#### Rerun cross-validation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pndFZSG78yQB","executionInfo":{"status":"ok","timestamp":1636724046240,"user_tz":0,"elapsed":49569,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}},"outputId":"af8c22e4-9f2b-456a-98b0-d1315fa371c5"},"source":["# loading reviews\n","# initialize global lists that will be appended to by the methods below\n","rawData = []\n","trainData = []\n","testData = []\n","\n","# input file location\n","reviewPath = './amazon_reviews.txt'\n","\n","## Run the functions\n","print(f\"Preparing the dataset\")\n","\n","# load the data\n","loadData(reviewPath) \n","\n","# split data into 80% training and 20% test\n","splitData(0.8, True, 1)\n","\n","print(f\"Number of reviews in raw data: {len(rawData)}\")\n","print(f\"Number of reviews in training data: {len(trainData)}\")\n","print(f\"Number of reviews in test data: {len(testData)}\")\n","\n","# cross validate on training data\n","cv_results = crossValidate(trainData)\n","# print the results\n","print(\"Results of cross validation:\")\n","print(\"Precision: %f\" % cv_results['precision'])\n","print(\"Recall: %f\" % cv_results['recall'])\n","print(\"F1 score: %f\" % cv_results['f1_score'])\n","print(\"Accuracy: %f\" % cv_results['accuracy'])"],"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Preparing the dataset\n","Number of reviews in raw data: 21000\n","Number of reviews in training data: 16800\n","Number of reviews in test data: 4200\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Results of cross validation:\n","Precision: 0.652599\n","Recall: 0.652024\n","F1 score: 0.651830\n","Accuracy: 0.652024\n"]}]},{"cell_type":"markdown","metadata":{"id":"BV_uogg-BBoX"},"source":["#### Evaluate using the test set\n","We can now train the classifier on 80% of the data and evaluate its performance using the 20% of the data we left out earlier."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEzNE5jvBBoX","executionInfo":{"status":"ok","timestamp":1636724047544,"user_tz":0,"elapsed":1309,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}},"outputId":"a6bcba9c-e636-482e-b5e4-4fdb9b602fd1"},"source":["# train the classifier using training data\n","classifier = trainClassifier(trainData)\n","# get the ground-truth labels from the data\n","testTrue = [t[1] for t in testData]\n","# classify the test data to get predicted labels   \n","testPred = predictLabels(testData, classifier) \n","# evaluate\n","finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted')\n","accuracy = accuracy_score(testTrue, testPred)\n","print(f\"Done training!\")\n","print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n","print(f\"Accuracy: {accuracy}\")"],"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Classifier...\n","Done training!\n","Precision: 0.636256\n","Recall: 0.636190\n","F Score:0.636147\n","Accuracy: 0.6361904761904762\n"]}]},{"cell_type":"markdown","metadata":{"id":"0furUXCZB2Ol"},"source":["### Looking beyond textual features of the review\n","As the review data contains additional features (rating, verified purchase, product category, product ID, product title, review title). I want to explore of using any of them can improve the performance of the classifier."]},{"cell_type":"markdown","metadata":{"id":"Fmpmi4cdCSl_"},"source":["Amended relevant functions below:"]},{"cell_type":"code","metadata":{"id":"129qmpqyA3s4","executionInfo":{"status":"ok","timestamp":1636724047545,"user_tz":0,"elapsed":9,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# amended loadData function\n","def loadData(path, Text=None,):\n","    with open(path) as f:\n","        reader = csv.reader(f, delimiter='\\t')\n","        \n","        for line in reader:\n","            if line[0] == \"DOC_ID\":  # skip the header\n","                continue\n","            # amend to include Verified, Category and ProductID\n","            (Id, Text, Label, Verified, Category, ProductID) = parseReview(line)\n","            rawData.append((Id, Text, Label, Verified, Category, ProductID))"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2d_lRonCXmd","executionInfo":{"status":"ok","timestamp":1636724047545,"user_tz":0,"elapsed":7,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# amended parseReview function\n","def parseReview(reviewLine):\n","    \n","    id = int(reviewLine[0])\n","    text = reviewLine[8]\n","    label_text = reviewLine[1]\n","    label = 'fake' if label_text == '__label1__' else 'real'\n","    # verified purchase as it is stored as Y or N, I want to convert\n","    # it to 1 or 0\n","    \n","    # amended to include verified, category and productID\n","    verified = 1 if reviewLine[3] == 'Y' else 0\n","    category = reviewLine[4]\n","    productID = reviewLine[5]\n","\n","    # the function now returnsn 6 values for each review\n","    return (id, text, label, verified, category, productID)"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOZGyA7SCYG1","executionInfo":{"status":"ok","timestamp":1636724047546,"user_tz":0,"elapsed":8,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# amended toFeatureVector_updated function\n","def toFeatureVector_updated(tokens, verified, category, productID):\n","\n","    # create an empty dictionary to be returned at the end (local dictionary)\n","    v = {}\n","    # for each token\n","    for t in tokens:\n","        # check if that token already exists in the local dictionary\n","        try:\n","            i = v[t]\n","            # if it does, increment the count\n","            v[t] = i + 1\n","        # otherwise, assign a count of 1\n","        except KeyError:\n","            v[t] = 1\n","\n","    # for every item in the dictionary, turn counts into weights\n","    v_weighted = {key:value/sum(v.values()) for (key, value) in v.items()}\n","    \n","    \n","    # amended so that the dictionary now returned the additional 3 features\n","    v_weighted['Verified'] = verified\n","    v_weighted['Category'] = category\n","    v_weighted['ProductID'] = productID\n","\n","    # return the local ditionary\n","    return v_weighted"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"VZUeBzMmCaii","executionInfo":{"status":"ok","timestamp":1636724047546,"user_tz":0,"elapsed":7,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}}},"source":["# amended splitData function\n","def splitData(percentage):\n","    # A method to split the data between trainData and testData \n","    dataSamples = len(rawData)\n","    halfOfData = int(len(rawData)/2)\n","    trainingSamples = int((percentage*dataSamples)/2)\n","\n","    # amended so that training and test datasets include the additional functions\n","    for (_, Text, Label, Verified, Category, ProductID) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n","        trainData.append((toFeatureVector_updated(preProcess_updated(Text),Verified, Category, ProductID),Label))\n","    for (_, Text, Label, Verified, Category, ProductID) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n","        testData.append((toFeatureVector_updated(preProcess_updated(Text),Verified, Category, ProductID),Label))"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krkz6KxUC2LS"},"source":["I found that the additional features that improved the performance were:\n","- verified purchase\n","- product category\n","- product id"]},{"cell_type":"markdown","metadata":{"id":"RakDcbjWCiFu"},"source":["#### Rerun cross-validation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kyd1RSp5Cci2","executionInfo":{"status":"ok","timestamp":1636724101109,"user_tz":0,"elapsed":53569,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}},"outputId":"bb6d1e64-b9b9-483d-d893-84a2e85c43f1"},"source":["# loading reviews\n","# initialize global lists that will be appended to by the methods below\n","rawData = []\n","trainData = []\n","testData = []\n","\n","# input file location\n","reviewPath = './amazon_reviews.txt'\n","\n","## Run the functions\n","print(f\"Preparing the dataset\")\n","\n","# load the data\n","loadData(reviewPath) \n","\n","# split data into 80% training and 20% test\n","splitData(0.8)\n","\n","print(f\"Number of reviews in raw data: {len(rawData)}\")\n","print(f\"Number of reviews in training data: {len(trainData)}\")\n","print(f\"Number of reviews in test data: {len(testData)}\")\n","\n","# cross validate on training data\n","cv_results = crossValidate(trainData)\n","# print the results\n","print(\"Results of cross validation:\")\n","print(\"Precision: %f\" % cv_results['precision'])\n","print(\"Recall: %f\" % cv_results['recall'])\n","print(\"F1 score: %f\" % cv_results['f1_score'])\n","print(\"Accuracy: %f\" % cv_results['accuracy'])"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Preparing the dataset\n","Number of reviews in raw data: 21000\n","Number of reviews in training data: 16800\n","Number of reviews in test data: 4200\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Training Classifier...\n","Results of cross validation:\n","Precision: 0.807451\n","Recall: 0.806905\n","F1 score: 0.806816\n","Accuracy: 0.806905\n"]}]},{"cell_type":"markdown","metadata":{"id":"6XXeftCPCs-k"},"source":["#### Evaluation on test data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Vp6WRDsCr-P","executionInfo":{"status":"ok","timestamp":1636724102815,"user_tz":0,"elapsed":1712,"user":{"displayName":"Maciej Tarsa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWk0j9fFSgnb3uzugIZcEXR3uc9Ze0OcOZznFdZA4=s64","userId":"17023749594418169578"}},"outputId":"f668de65-f394-4154-e31e-15cfdc335a08"},"source":["# train the classifier using training data\n","classifier = trainClassifier(trainData)\n","# get the ground-truth labels from the data\n","testTrue = [t[1] for t in testData]\n","# classify the test data to get predicted labels   \n","testPred = predictLabels(testData, classifier) \n","# evaluate\n","finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted')\n","accuracy = accuracy_score(testTrue, testPred)\n","print(f\"Done training!\")\n","print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n","print(f\"Accuracy: {accuracy}\")"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Classifier...\n","Done training!\n","Precision: 0.818593\n","Recall: 0.815000\n","F Score:0.814477\n","Accuracy: 0.815\n"]}]},{"cell_type":"markdown","metadata":{"id":"7T3C5z55D0fw"},"source":["## Conclusions"]},{"cell_type":"markdown","metadata":{"id":"bh6mi6LZD1-u"},"source":["Below, I present a list of results by running the model on the test dataset using the base functions, amended pre-processing and using additonal features. Each time the model is run, the results might be slightly different as the model splits the whole dataset into training and test at random.\n","\n","| Metrics | Base functions | Additional pre-processing | Additional features |\n","| --- | --- | --- | --- |\n","| Precision | 0.6036 | 0.6363 | 0.8186 |\n","| Recall | 0.6036 | 0.6362 | 0.8150 |\n","| F1 Score | 0.6036 | 0.6361 | 0.8144 |\n","| Accuracy | 0.6036 | 0.6362 | 0.815 |\n","\n","Overall, the improvement is significant, particularly when additional features not from the text of the review were included. The performance of the classifier started ar around 60% and improved to over 81%, an improvement of over 21%."]},{"cell_type":"markdown","metadata":{"id":"pUSwYpJjD4qg"},"source":["The full list of improvements made:\n","- stemming of tokens\n","- lemmatisation of tokens\n","- feature weighing of vectorised tokens\n","- inclusion of 'Verified purchase' as a feature\n","- inclusion of 'Product category' as a feature\n","- inclusion of 'Product ID' as a feature"]}]}